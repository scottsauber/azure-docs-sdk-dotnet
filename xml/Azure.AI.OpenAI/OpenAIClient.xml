<Type Name="OpenAIClient" FullName="Azure.AI.OpenAI.OpenAIClient">
  <TypeSignature Language="C#" Value="public class OpenAIClient" />
  <TypeSignature Language="ILAsm" Value=".class public auto ansi beforefieldinit OpenAIClient extends System.Object" />
  <TypeSignature Language="DocId" Value="T:Azure.AI.OpenAI.OpenAIClient" />
  <TypeSignature Language="VB.NET" Value="Public Class OpenAIClient" />
  <TypeSignature Language="F#" Value="type OpenAIClient = class" />
  <AssemblyInfo>
    <AssemblyName>Azure.AI.OpenAI</AssemblyName>
    <AssemblyVersion>1.0.0.0</AssemblyVersion>
  </AssemblyInfo>
  <Base>
    <BaseTypeName>System.Object</BaseTypeName>
  </Base>
  <Interfaces />
  <Docs>
    <summary> Azure OpenAI APIs for completions and search. </summary>
    <remarks>To be added.</remarks>
  </Docs>
  <Members>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="protected OpenAIClient ();" />
      <MemberSignature Language="ILAsm" Value=".method familyhidebysig specialname rtspecialname instance void .ctor() cil managed" />
      <MemberSignature Language="DocId" Value="M:Azure.AI.OpenAI.OpenAIClient.#ctor" />
      <MemberSignature Language="VB.NET" Value="Protected Sub New ()" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>Azure.AI.OpenAI</AssemblyName>
        <AssemblyVersion>1.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters />
      <Docs>
        <summary> Initializes a new instance of OpenAIClient for mocking. </summary>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public OpenAIClient (Uri endpoint, Azure.AzureKeyCredential credential);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(class System.Uri endpoint, class Azure.AzureKeyCredential credential) cil managed" />
      <MemberSignature Language="DocId" Value="M:Azure.AI.OpenAI.OpenAIClient.#ctor(System.Uri,Azure.AzureKeyCredential)" />
      <MemberSignature Language="VB.NET" Value="Public Sub New (endpoint As Uri, credential As AzureKeyCredential)" />
      <MemberSignature Language="F#" Value="new Azure.AI.OpenAI.OpenAIClient : Uri * Azure.AzureKeyCredential -&gt; Azure.AI.OpenAI.OpenAIClient" Usage="new Azure.AI.OpenAI.OpenAIClient (endpoint, credential)" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>Azure.AI.OpenAI</AssemblyName>
        <AssemblyVersion>1.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="endpoint" Type="System.Uri" />
        <Parameter Name="credential" Type="Azure.AzureKeyCredential" />
      </Parameters>
      <Docs>
        <param name="endpoint">
            Supported Cognitive Services endpoints (protocol and hostname, for example:
            https://westus.api.cognitive.microsoft.com).
            </param>
        <param name="credential"> A credential used to authenticate to an Azure Service. </param>
        <summary> Initializes a new instance of OpenAIClient. </summary>
        <remarks>To be added.</remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="endpoint" /> or <paramref name="credential" /> is null. </exception>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public OpenAIClient (Uri endpoint, Azure.Core.TokenCredential credential);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(class System.Uri endpoint, class Azure.Core.TokenCredential credential) cil managed" />
      <MemberSignature Language="DocId" Value="M:Azure.AI.OpenAI.OpenAIClient.#ctor(System.Uri,Azure.Core.TokenCredential)" />
      <MemberSignature Language="VB.NET" Value="Public Sub New (endpoint As Uri, credential As TokenCredential)" />
      <MemberSignature Language="F#" Value="new Azure.AI.OpenAI.OpenAIClient : Uri * Azure.Core.TokenCredential -&gt; Azure.AI.OpenAI.OpenAIClient" Usage="new Azure.AI.OpenAI.OpenAIClient (endpoint, credential)" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>Azure.AI.OpenAI</AssemblyName>
        <AssemblyVersion>1.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="endpoint" Type="System.Uri" />
        <Parameter Name="credential" Type="Azure.Core.TokenCredential" />
      </Parameters>
      <Docs>
        <param name="endpoint">
            Supported Cognitive Services endpoints (protocol and hostname, for example:
            https://westus.api.cognitive.microsoft.com).
            </param>
        <param name="credential"> A credential used to authenticate to an Azure Service. </param>
        <summary> Initializes a new instance of OpenAIClient. </summary>
        <remarks>To be added.</remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="endpoint" /> or <paramref name="credential" /> is null. </exception>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public OpenAIClient (Uri endpoint, Azure.AzureKeyCredential credential, Azure.AI.OpenAI.OpenAIClientOptions options);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(class System.Uri endpoint, class Azure.AzureKeyCredential credential, class Azure.AI.OpenAI.OpenAIClientOptions options) cil managed" />
      <MemberSignature Language="DocId" Value="M:Azure.AI.OpenAI.OpenAIClient.#ctor(System.Uri,Azure.AzureKeyCredential,Azure.AI.OpenAI.OpenAIClientOptions)" />
      <MemberSignature Language="VB.NET" Value="Public Sub New (endpoint As Uri, credential As AzureKeyCredential, options As OpenAIClientOptions)" />
      <MemberSignature Language="F#" Value="new Azure.AI.OpenAI.OpenAIClient : Uri * Azure.AzureKeyCredential * Azure.AI.OpenAI.OpenAIClientOptions -&gt; Azure.AI.OpenAI.OpenAIClient" Usage="new Azure.AI.OpenAI.OpenAIClient (endpoint, credential, options)" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>Azure.AI.OpenAI</AssemblyName>
        <AssemblyVersion>1.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="endpoint" Type="System.Uri" />
        <Parameter Name="credential" Type="Azure.AzureKeyCredential" />
        <Parameter Name="options" Type="Azure.AI.OpenAI.OpenAIClientOptions" />
      </Parameters>
      <Docs>
        <param name="endpoint">
            Supported Cognitive Services endpoints (protocol and hostname, for example:
            https://westus.api.cognitive.microsoft.com).
            </param>
        <param name="credential"> A credential used to authenticate to an Azure Service. </param>
        <param name="options"> The options for configuring the client. </param>
        <summary> Initializes a new instance of OpenAIClient. </summary>
        <remarks>To be added.</remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="endpoint" /> or <paramref name="credential" /> is null. </exception>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public OpenAIClient (Uri endpoint, Azure.Core.TokenCredential credential, Azure.AI.OpenAI.OpenAIClientOptions options);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(class System.Uri endpoint, class Azure.Core.TokenCredential credential, class Azure.AI.OpenAI.OpenAIClientOptions options) cil managed" />
      <MemberSignature Language="DocId" Value="M:Azure.AI.OpenAI.OpenAIClient.#ctor(System.Uri,Azure.Core.TokenCredential,Azure.AI.OpenAI.OpenAIClientOptions)" />
      <MemberSignature Language="VB.NET" Value="Public Sub New (endpoint As Uri, credential As TokenCredential, options As OpenAIClientOptions)" />
      <MemberSignature Language="F#" Value="new Azure.AI.OpenAI.OpenAIClient : Uri * Azure.Core.TokenCredential * Azure.AI.OpenAI.OpenAIClientOptions -&gt; Azure.AI.OpenAI.OpenAIClient" Usage="new Azure.AI.OpenAI.OpenAIClient (endpoint, credential, options)" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>Azure.AI.OpenAI</AssemblyName>
        <AssemblyVersion>1.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="endpoint" Type="System.Uri" />
        <Parameter Name="credential" Type="Azure.Core.TokenCredential" />
        <Parameter Name="options" Type="Azure.AI.OpenAI.OpenAIClientOptions" />
      </Parameters>
      <Docs>
        <param name="endpoint">
            Supported Cognitive Services endpoints (protocol and hostname, for example:
            https://westus.api.cognitive.microsoft.com).
            </param>
        <param name="credential"> A credential used to authenticate to an Azure Service. </param>
        <param name="options"> The options for configuring the client. </param>
        <summary> Initializes a new instance of OpenAIClient. </summary>
        <remarks>To be added.</remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="endpoint" /> or <paramref name="credential" /> is null. </exception>
      </Docs>
    </Member>
    <Member MemberName="GetCompletions">
      <MemberSignature Language="C#" Value="public virtual Azure.Response&lt;Azure.AI.OpenAI.Completions&gt; GetCompletions (string deploymentId, Azure.AI.OpenAI.CompletionsOptions completionsOptions, System.Threading.CancellationToken cancellationToken = default);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig newslot virtual instance class Azure.Response`1&lt;class Azure.AI.OpenAI.Completions&gt; GetCompletions(string deploymentId, class Azure.AI.OpenAI.CompletionsOptions completionsOptions, valuetype System.Threading.CancellationToken cancellationToken) cil managed" />
      <MemberSignature Language="DocId" Value="M:Azure.AI.OpenAI.OpenAIClient.GetCompletions(System.String,Azure.AI.OpenAI.CompletionsOptions,System.Threading.CancellationToken)" />
      <MemberSignature Language="VB.NET" Value="Public Overridable Function GetCompletions (deploymentId As String, completionsOptions As CompletionsOptions, Optional cancellationToken As CancellationToken = Nothing) As Response(Of Completions)" />
      <MemberSignature Language="F#" Value="abstract member GetCompletions : string * Azure.AI.OpenAI.CompletionsOptions * System.Threading.CancellationToken -&gt; Azure.Response&lt;Azure.AI.OpenAI.Completions&gt;&#xA;override this.GetCompletions : string * Azure.AI.OpenAI.CompletionsOptions * System.Threading.CancellationToken -&gt; Azure.Response&lt;Azure.AI.OpenAI.Completions&gt;" Usage="openAIClient.GetCompletions (deploymentId, completionsOptions, cancellationToken)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>Azure.AI.OpenAI</AssemblyName>
        <AssemblyVersion>1.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>Azure.Response&lt;Azure.AI.OpenAI.Completions&gt;</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="deploymentId" Type="System.String" />
        <Parameter Name="completionsOptions" Type="Azure.AI.OpenAI.CompletionsOptions" />
        <Parameter Name="cancellationToken" Type="System.Threading.CancellationToken" />
      </Parameters>
      <Docs>
        <param name="deploymentId"> deployment id of the deployed model. </param>
        <param name="completionsOptions"> Post body schema to create a prompt completion from a deployment. </param>
        <param name="cancellationToken"> The cancellation token to use. </param>
        <summary> Return the completions for a given prompt. </summary>
        <returns>To be added.</returns>
        <remarks>To be added.</remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="deploymentId" /> or <paramref name="completionsOptions" /> is null. </exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="deploymentId" /> is an empty string, and was expected to be non-empty. </exception>
      </Docs>
    </Member>
    <Member MemberName="GetCompletions">
      <MemberSignature Language="C#" Value="public virtual Azure.Response GetCompletions (string deploymentId, Azure.Core.RequestContent content, Azure.RequestContext context = default);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig newslot virtual instance class Azure.Response GetCompletions(string deploymentId, class Azure.Core.RequestContent content, class Azure.RequestContext context) cil managed" />
      <MemberSignature Language="DocId" Value="M:Azure.AI.OpenAI.OpenAIClient.GetCompletions(System.String,Azure.Core.RequestContent,Azure.RequestContext)" />
      <MemberSignature Language="VB.NET" Value="Public Overridable Function GetCompletions (deploymentId As String, content As RequestContent, Optional context As RequestContext = Nothing) As Response" />
      <MemberSignature Language="F#" Value="abstract member GetCompletions : string * Azure.Core.RequestContent * Azure.RequestContext -&gt; Azure.Response&#xA;override this.GetCompletions : string * Azure.Core.RequestContent * Azure.RequestContext -&gt; Azure.Response" Usage="openAIClient.GetCompletions (deploymentId, content, context)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>Azure.AI.OpenAI</AssemblyName>
        <AssemblyVersion>1.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>Azure.Response</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="deploymentId" Type="System.String" />
        <Parameter Name="content" Type="Azure.Core.RequestContent" />
        <Parameter Name="context" Type="Azure.RequestContext" />
      </Parameters>
      <Docs>
        <param name="deploymentId"> deployment id of the deployed model. </param>
        <param name="content"> The content to send as the body of the request. Details of the request body schema are in the Remarks section below. </param>
        <param name="context"> The request context, which can override default behaviors of the client pipeline on a per-call basis. </param>
        <summary> Return the completions for a given prompt. </summary>
        <returns> The response returned from the service. Details of the response body schema are in the Remarks section below. </returns>
        <remarks>
Below is the JSON schema for the request and response payloads.

Request Body:

Schema for <c>CompletionsOptions</c>:
<code>{
  prompt: [string], # Optional. An optional prompt to complete from, encoded as a string, a list of strings, or
a list of token lists. Defaults to &amp;lt;|endoftext|&amp;gt;. The prompt to complete from.
If you would like to provide multiple prompts, use the POST variant of this
method. Note that &amp;lt;|endoftext|&amp;gt; is the document separator that the model sees
during training, so if a prompt is not specified the model will generate as if
from the beginning of a new document. Maximum allowed size of string list is
2048.
  max_tokens: number, # Optional. The maximum number of tokens to generate. Has minimum of 0.
  temperature: number, # Optional. What sampling temperature to use. Higher values means the model will take more
risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones
with a well-defined answer.
We generally recommend using this or `top_p` but
not both.
Minimum of 0 and maximum of 2 allowed.

  top_p: number, # Optional. An alternative to sampling with temperature, called nucleus sampling, where the
model considers the results of the tokens with top_p probability mass. So 0.1
means only the tokens comprising the top 10% probability mass are
considered.
We generally recommend using this or `temperature` but not
both.
Minimum of 0 and maximum of 1 allowed.

  logit_bias: Dictionary&lt;string, number&gt;, # Optional. Defaults to null. Modify the likelihood of specified tokens appearing in the
completion. Accepts a json object that maps tokens (specified by their token ID
in the GPT tokenizer) to an associated bias value from -100 to 100. You can use
this tokenizer tool (which works for both GPT-2 and GPT-3) to convert text to
token IDs. Mathematically, the bias is added to the logits generated by the
model prior to sampling. The exact effect will vary per model, but values
between -1 and 1 should decrease or increase likelihood of selection; values
like -100 or 100 should result in a ban or exclusive selection of the relevant
token. As an example, you can pass {"50256" &amp;amp;#58; -100} to prevent the
&amp;lt;|endoftext|&amp;gt; token from being generated.
  user: string, # Optional. The ID of the end-user, for use in tracking and rate-limiting.
  n: number, # Optional. How many snippets to generate for each prompt. Minimum of 1 and maximum of 128
allowed.
  logprobs: number, # Optional. Include the log probabilities on the `logprobs` most likely tokens, as well the
chosen tokens. So for example, if `logprobs` is 10, the API will return a list
of the 10 most likely tokens. If `logprobs` is 0, only the chosen tokens will
have logprobs returned. Minimum of 0 and maximum of 100 allowed.
  model: string, # Optional. The name of the model to use
  echo: boolean, # Optional. Echo back the prompt in addition to the completion
  stop: [string], # Optional. A sequence which indicates the end of the current document.
  completion_config: string, # Optional. Completion configuration
  cache_level: number, # Optional. can be used to disable any server-side caching, 0=no cache, 1=prompt prefix
enabled, 2=full cache
  presence_penalty: number, # Optional. How much to penalize new tokens based on their existing frequency in the text
so far. Decreases the model's likelihood to repeat the same line verbatim. Has
minimum of -2 and maximum of 2.
  frequency_penalty: number, # Optional. How much to penalize new tokens based on whether they appear in the text so
far. Increases the model's likelihood to talk about new topics.
  best_of: number, # Optional. How many generations to create server side, and display only the best. Will not
stream intermediate progress if best_of &amp;gt; 1. Has maximum value of 128.
}
</code>

Response Body:

Schema for <c>Completions</c>:
<code>{
  id: string, # Optional. Id for completion response
  object: Literal, # Required. Object for completion response
  created: number, # Optional. Created time for completion response
  model: string, # Optional. Model used for completion response
  choices: [
    {
      text: string, # Optional. Generated text for given completion prompt
      index: number, # Optional. Index
      logprobs: {
        tokens: [string], # Optional. Tokens
        token_logprobs: [number], # Optional. LogProbs of Tokens
        top_logprobs: [Dictionary&lt;string, number&gt;], # Optional. Top LogProbs
        text_offset: [number], # Optional. Text offset
      }, # Optional. Log Prob Model
      finish_reason: string, # Optional. Reason for finishing
    }
  ], # Optional. Array of choices returned containing text completions to prompts sent
  usage: {
    completion_tokens: number, # Required. Number of tokens received in the completion
    prompt_tokens: number, # Required. Number of tokens sent in the original request
    total_tokens: number, # Required. Total number of tokens transacted in this request/response
  }, # Required. Usage counts for tokens input using the completions API
}
</code></remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="deploymentId" /> or <paramref name="content" /> is null. </exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="deploymentId" /> is an empty string, and was expected to be non-empty. </exception>
        <exception cref="T:Azure.RequestFailedException"> Service returned a non-success status code. </exception>
        <example>
This sample shows how to call GetCompletions with required parameters and parse the result.
<code><![CDATA[
var credential = new AzureKeyCredential("<key>");
var endpoint = new Uri("<https://my-service.azure.com>");
var client = new OpenAIClient(endpoint, credential);

var data = new {};

Response response = client.GetCompletions("<deploymentId>", RequestContent.Create(data));

JsonElement result = JsonDocument.Parse(response.ContentStream).RootElement;
Console.WriteLine(result.GetProperty("object").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("completion_tokens").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("prompt_tokens").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("total_tokens").ToString());
]]></code>
This sample shows how to call GetCompletions with all parameters and request content, and how to parse the result.
<code><![CDATA[
var credential = new AzureKeyCredential("<key>");
var endpoint = new Uri("<https://my-service.azure.com>");
var client = new OpenAIClient(endpoint, credential);

var data = new {
    prompt = new[] {
        "<String>"
    },
    max_tokens = 1234,
    temperature = 123.45f,
    top_p = 123.45f,
    logit_bias = new {
        key = 1234,
    },
    user = "<user>",
    n = 1234,
    logprobs = 1234,
    model = "<model>",
    echo = true,
    stop = new[] {
        "<String>"
    },
    completion_config = "<completion_config>",
    cache_level = 1234,
    presence_penalty = 123.45f,
    frequency_penalty = 123.45f,
    best_of = 1234,
};

Response response = client.GetCompletions("<deploymentId>", RequestContent.Create(data));

JsonElement result = JsonDocument.Parse(response.ContentStream).RootElement;
Console.WriteLine(result.GetProperty("id").ToString());
Console.WriteLine(result.GetProperty("object").ToString());
Console.WriteLine(result.GetProperty("created").ToString());
Console.WriteLine(result.GetProperty("model").ToString());
Console.WriteLine(result.GetProperty("choices")[0].GetProperty("text").ToString());
Console.WriteLine(result.GetProperty("choices")[0].GetProperty("index").ToString());
Console.WriteLine(result.GetProperty("choices")[0].GetProperty("logprobs").GetProperty("tokens")[0].ToString());
Console.WriteLine(result.GetProperty("choices")[0].GetProperty("logprobs").GetProperty("token_logprobs")[0].ToString());
Console.WriteLine(result.GetProperty("choices")[0].GetProperty("logprobs").GetProperty("top_logprobs")[0].GetProperty("<test>").ToString());
Console.WriteLine(result.GetProperty("choices")[0].GetProperty("logprobs").GetProperty("text_offset")[0].ToString());
Console.WriteLine(result.GetProperty("choices")[0].GetProperty("finish_reason").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("completion_tokens").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("prompt_tokens").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("total_tokens").ToString());
]]></code></example>
      </Docs>
    </Member>
    <Member MemberName="GetCompletions">
      <MemberSignature Language="C#" Value="public virtual Azure.Response&lt;Azure.AI.OpenAI.Completions&gt; GetCompletions (string deploymentId, string prompt, System.Threading.CancellationToken cancellationToken = default);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig newslot virtual instance class Azure.Response`1&lt;class Azure.AI.OpenAI.Completions&gt; GetCompletions(string deploymentId, string prompt, valuetype System.Threading.CancellationToken cancellationToken) cil managed" />
      <MemberSignature Language="DocId" Value="M:Azure.AI.OpenAI.OpenAIClient.GetCompletions(System.String,System.String,System.Threading.CancellationToken)" />
      <MemberSignature Language="VB.NET" Value="Public Overridable Function GetCompletions (deploymentId As String, prompt As String, Optional cancellationToken As CancellationToken = Nothing) As Response(Of Completions)" />
      <MemberSignature Language="F#" Value="abstract member GetCompletions : string * string * System.Threading.CancellationToken -&gt; Azure.Response&lt;Azure.AI.OpenAI.Completions&gt;&#xA;override this.GetCompletions : string * string * System.Threading.CancellationToken -&gt; Azure.Response&lt;Azure.AI.OpenAI.Completions&gt;" Usage="openAIClient.GetCompletions (deploymentId, prompt, cancellationToken)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>Azure.AI.OpenAI</AssemblyName>
        <AssemblyVersion>1.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>Azure.Response&lt;Azure.AI.OpenAI.Completions&gt;</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="deploymentId" Type="System.String" />
        <Parameter Name="prompt" Type="System.String" />
        <Parameter Name="cancellationToken" Type="System.Threading.CancellationToken" />
      </Parameters>
      <Docs>
        <param name="deploymentId"> Deployment id (also known as model name) to use for operations </param>
        <param name="prompt"> Input string prompt to create a prompt completion from a deployment. </param>
        <param name="cancellationToken"> The cancellation token to use. </param>
        <summary> Return the completions for a given prompt. </summary>
        <returns>To be added.</returns>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <Member MemberName="GetCompletionsAsync">
      <MemberSignature Language="C#" Value="public virtual System.Threading.Tasks.Task&lt;Azure.Response&lt;Azure.AI.OpenAI.Completions&gt;&gt; GetCompletionsAsync (string deploymentId, Azure.AI.OpenAI.CompletionsOptions completionsOptions, System.Threading.CancellationToken cancellationToken = default);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig newslot virtual instance class System.Threading.Tasks.Task`1&lt;class Azure.Response`1&lt;class Azure.AI.OpenAI.Completions&gt;&gt; GetCompletionsAsync(string deploymentId, class Azure.AI.OpenAI.CompletionsOptions completionsOptions, valuetype System.Threading.CancellationToken cancellationToken) cil managed" />
      <MemberSignature Language="DocId" Value="M:Azure.AI.OpenAI.OpenAIClient.GetCompletionsAsync(System.String,Azure.AI.OpenAI.CompletionsOptions,System.Threading.CancellationToken)" />
      <MemberSignature Language="VB.NET" Value="Public Overridable Function GetCompletionsAsync (deploymentId As String, completionsOptions As CompletionsOptions, Optional cancellationToken As CancellationToken = Nothing) As Task(Of Response(Of Completions))" />
      <MemberSignature Language="F#" Value="abstract member GetCompletionsAsync : string * Azure.AI.OpenAI.CompletionsOptions * System.Threading.CancellationToken -&gt; System.Threading.Tasks.Task&lt;Azure.Response&lt;Azure.AI.OpenAI.Completions&gt;&gt;&#xA;override this.GetCompletionsAsync : string * Azure.AI.OpenAI.CompletionsOptions * System.Threading.CancellationToken -&gt; System.Threading.Tasks.Task&lt;Azure.Response&lt;Azure.AI.OpenAI.Completions&gt;&gt;" Usage="openAIClient.GetCompletionsAsync (deploymentId, completionsOptions, cancellationToken)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>Azure.AI.OpenAI</AssemblyName>
        <AssemblyVersion>1.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Threading.Tasks.Task&lt;Azure.Response&lt;Azure.AI.OpenAI.Completions&gt;&gt;</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="deploymentId" Type="System.String" />
        <Parameter Name="completionsOptions" Type="Azure.AI.OpenAI.CompletionsOptions" />
        <Parameter Name="cancellationToken" Type="System.Threading.CancellationToken" />
      </Parameters>
      <Docs>
        <param name="deploymentId"> deployment id of the deployed model. </param>
        <param name="completionsOptions"> Post body schema to create a prompt completion from a deployment. </param>
        <param name="cancellationToken"> The cancellation token to use. </param>
        <summary> Return the completions for a given prompt. </summary>
        <returns>To be added.</returns>
        <remarks>To be added.</remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="deploymentId" /> or <paramref name="completionsOptions" /> is null. </exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="deploymentId" /> is an empty string, and was expected to be non-empty. </exception>
      </Docs>
    </Member>
    <Member MemberName="GetCompletionsAsync">
      <MemberSignature Language="C#" Value="public virtual System.Threading.Tasks.Task&lt;Azure.Response&gt; GetCompletionsAsync (string deploymentId, Azure.Core.RequestContent content, Azure.RequestContext context = default);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig newslot virtual instance class System.Threading.Tasks.Task`1&lt;class Azure.Response&gt; GetCompletionsAsync(string deploymentId, class Azure.Core.RequestContent content, class Azure.RequestContext context) cil managed" />
      <MemberSignature Language="DocId" Value="M:Azure.AI.OpenAI.OpenAIClient.GetCompletionsAsync(System.String,Azure.Core.RequestContent,Azure.RequestContext)" />
      <MemberSignature Language="VB.NET" Value="Public Overridable Function GetCompletionsAsync (deploymentId As String, content As RequestContent, Optional context As RequestContext = Nothing) As Task(Of Response)" />
      <MemberSignature Language="F#" Value="abstract member GetCompletionsAsync : string * Azure.Core.RequestContent * Azure.RequestContext -&gt; System.Threading.Tasks.Task&lt;Azure.Response&gt;&#xA;override this.GetCompletionsAsync : string * Azure.Core.RequestContent * Azure.RequestContext -&gt; System.Threading.Tasks.Task&lt;Azure.Response&gt;" Usage="openAIClient.GetCompletionsAsync (deploymentId, content, context)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>Azure.AI.OpenAI</AssemblyName>
        <AssemblyVersion>1.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Threading.Tasks.Task&lt;Azure.Response&gt;</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="deploymentId" Type="System.String" />
        <Parameter Name="content" Type="Azure.Core.RequestContent" />
        <Parameter Name="context" Type="Azure.RequestContext" />
      </Parameters>
      <Docs>
        <param name="deploymentId"> deployment id of the deployed model. </param>
        <param name="content"> The content to send as the body of the request. Details of the request body schema are in the Remarks section below. </param>
        <param name="context"> The request context, which can override default behaviors of the client pipeline on a per-call basis. </param>
        <summary> Return the completions for a given prompt. </summary>
        <returns> The response returned from the service. Details of the response body schema are in the Remarks section below. </returns>
        <remarks>
Below is the JSON schema for the request and response payloads.

Request Body:

Schema for <c>CompletionsOptions</c>:
<code>{
  prompt: [string], # Optional. An optional prompt to complete from, encoded as a string, a list of strings, or
a list of token lists. Defaults to &amp;lt;|endoftext|&amp;gt;. The prompt to complete from.
If you would like to provide multiple prompts, use the POST variant of this
method. Note that &amp;lt;|endoftext|&amp;gt; is the document separator that the model sees
during training, so if a prompt is not specified the model will generate as if
from the beginning of a new document. Maximum allowed size of string list is
2048.
  max_tokens: number, # Optional. The maximum number of tokens to generate. Has minimum of 0.
  temperature: number, # Optional. What sampling temperature to use. Higher values means the model will take more
risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones
with a well-defined answer.
We generally recommend using this or `top_p` but
not both.
Minimum of 0 and maximum of 2 allowed.

  top_p: number, # Optional. An alternative to sampling with temperature, called nucleus sampling, where the
model considers the results of the tokens with top_p probability mass. So 0.1
means only the tokens comprising the top 10% probability mass are
considered.
We generally recommend using this or `temperature` but not
both.
Minimum of 0 and maximum of 1 allowed.

  logit_bias: Dictionary&lt;string, number&gt;, # Optional. Defaults to null. Modify the likelihood of specified tokens appearing in the
completion. Accepts a json object that maps tokens (specified by their token ID
in the GPT tokenizer) to an associated bias value from -100 to 100. You can use
this tokenizer tool (which works for both GPT-2 and GPT-3) to convert text to
token IDs. Mathematically, the bias is added to the logits generated by the
model prior to sampling. The exact effect will vary per model, but values
between -1 and 1 should decrease or increase likelihood of selection; values
like -100 or 100 should result in a ban or exclusive selection of the relevant
token. As an example, you can pass {"50256" &amp;amp;#58; -100} to prevent the
&amp;lt;|endoftext|&amp;gt; token from being generated.
  user: string, # Optional. The ID of the end-user, for use in tracking and rate-limiting.
  n: number, # Optional. How many snippets to generate for each prompt. Minimum of 1 and maximum of 128
allowed.
  logprobs: number, # Optional. Include the log probabilities on the `logprobs` most likely tokens, as well the
chosen tokens. So for example, if `logprobs` is 10, the API will return a list
of the 10 most likely tokens. If `logprobs` is 0, only the chosen tokens will
have logprobs returned. Minimum of 0 and maximum of 100 allowed.
  model: string, # Optional. The name of the model to use
  echo: boolean, # Optional. Echo back the prompt in addition to the completion
  stop: [string], # Optional. A sequence which indicates the end of the current document.
  completion_config: string, # Optional. Completion configuration
  cache_level: number, # Optional. can be used to disable any server-side caching, 0=no cache, 1=prompt prefix
enabled, 2=full cache
  presence_penalty: number, # Optional. How much to penalize new tokens based on their existing frequency in the text
so far. Decreases the model's likelihood to repeat the same line verbatim. Has
minimum of -2 and maximum of 2.
  frequency_penalty: number, # Optional. How much to penalize new tokens based on whether they appear in the text so
far. Increases the model's likelihood to talk about new topics.
  best_of: number, # Optional. How many generations to create server side, and display only the best. Will not
stream intermediate progress if best_of &amp;gt; 1. Has maximum value of 128.
}
</code>

Response Body:

Schema for <c>Completions</c>:
<code>{
  id: string, # Optional. Id for completion response
  object: Literal, # Required. Object for completion response
  created: number, # Optional. Created time for completion response
  model: string, # Optional. Model used for completion response
  choices: [
    {
      text: string, # Optional. Generated text for given completion prompt
      index: number, # Optional. Index
      logprobs: {
        tokens: [string], # Optional. Tokens
        token_logprobs: [number], # Optional. LogProbs of Tokens
        top_logprobs: [Dictionary&lt;string, number&gt;], # Optional. Top LogProbs
        text_offset: [number], # Optional. Text offset
      }, # Optional. Log Prob Model
      finish_reason: string, # Optional. Reason for finishing
    }
  ], # Optional. Array of choices returned containing text completions to prompts sent
  usage: {
    completion_tokens: number, # Required. Number of tokens received in the completion
    prompt_tokens: number, # Required. Number of tokens sent in the original request
    total_tokens: number, # Required. Total number of tokens transacted in this request/response
  }, # Required. Usage counts for tokens input using the completions API
}
</code></remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="deploymentId" /> or <paramref name="content" /> is null. </exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="deploymentId" /> is an empty string, and was expected to be non-empty. </exception>
        <exception cref="T:Azure.RequestFailedException"> Service returned a non-success status code. </exception>
        <example>
This sample shows how to call GetCompletionsAsync with required parameters and parse the result.
<code><![CDATA[
var credential = new AzureKeyCredential("<key>");
var endpoint = new Uri("<https://my-service.azure.com>");
var client = new OpenAIClient(endpoint, credential);

var data = new {};

Response response = await client.GetCompletionsAsync("<deploymentId>", RequestContent.Create(data));

JsonElement result = JsonDocument.Parse(response.ContentStream).RootElement;
Console.WriteLine(result.GetProperty("object").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("completion_tokens").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("prompt_tokens").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("total_tokens").ToString());
]]></code>
This sample shows how to call GetCompletionsAsync with all parameters and request content, and how to parse the result.
<code><![CDATA[
var credential = new AzureKeyCredential("<key>");
var endpoint = new Uri("<https://my-service.azure.com>");
var client = new OpenAIClient(endpoint, credential);

var data = new {
    prompt = new[] {
        "<String>"
    },
    max_tokens = 1234,
    temperature = 123.45f,
    top_p = 123.45f,
    logit_bias = new {
        key = 1234,
    },
    user = "<user>",
    n = 1234,
    logprobs = 1234,
    model = "<model>",
    echo = true,
    stop = new[] {
        "<String>"
    },
    completion_config = "<completion_config>",
    cache_level = 1234,
    presence_penalty = 123.45f,
    frequency_penalty = 123.45f,
    best_of = 1234,
};

Response response = await client.GetCompletionsAsync("<deploymentId>", RequestContent.Create(data));

JsonElement result = JsonDocument.Parse(response.ContentStream).RootElement;
Console.WriteLine(result.GetProperty("id").ToString());
Console.WriteLine(result.GetProperty("object").ToString());
Console.WriteLine(result.GetProperty("created").ToString());
Console.WriteLine(result.GetProperty("model").ToString());
Console.WriteLine(result.GetProperty("choices")[0].GetProperty("text").ToString());
Console.WriteLine(result.GetProperty("choices")[0].GetProperty("index").ToString());
Console.WriteLine(result.GetProperty("choices")[0].GetProperty("logprobs").GetProperty("tokens")[0].ToString());
Console.WriteLine(result.GetProperty("choices")[0].GetProperty("logprobs").GetProperty("token_logprobs")[0].ToString());
Console.WriteLine(result.GetProperty("choices")[0].GetProperty("logprobs").GetProperty("top_logprobs")[0].GetProperty("<test>").ToString());
Console.WriteLine(result.GetProperty("choices")[0].GetProperty("logprobs").GetProperty("text_offset")[0].ToString());
Console.WriteLine(result.GetProperty("choices")[0].GetProperty("finish_reason").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("completion_tokens").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("prompt_tokens").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("total_tokens").ToString());
]]></code></example>
      </Docs>
    </Member>
    <Member MemberName="GetCompletionsAsync">
      <MemberSignature Language="C#" Value="public virtual System.Threading.Tasks.Task&lt;Azure.Response&lt;Azure.AI.OpenAI.Completions&gt;&gt; GetCompletionsAsync (string deploymentId, string prompt, System.Threading.CancellationToken cancellationToken = default);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig newslot virtual instance class System.Threading.Tasks.Task`1&lt;class Azure.Response`1&lt;class Azure.AI.OpenAI.Completions&gt;&gt; GetCompletionsAsync(string deploymentId, string prompt, valuetype System.Threading.CancellationToken cancellationToken) cil managed" />
      <MemberSignature Language="DocId" Value="M:Azure.AI.OpenAI.OpenAIClient.GetCompletionsAsync(System.String,System.String,System.Threading.CancellationToken)" />
      <MemberSignature Language="VB.NET" Value="Public Overridable Function GetCompletionsAsync (deploymentId As String, prompt As String, Optional cancellationToken As CancellationToken = Nothing) As Task(Of Response(Of Completions))" />
      <MemberSignature Language="F#" Value="abstract member GetCompletionsAsync : string * string * System.Threading.CancellationToken -&gt; System.Threading.Tasks.Task&lt;Azure.Response&lt;Azure.AI.OpenAI.Completions&gt;&gt;&#xA;override this.GetCompletionsAsync : string * string * System.Threading.CancellationToken -&gt; System.Threading.Tasks.Task&lt;Azure.Response&lt;Azure.AI.OpenAI.Completions&gt;&gt;" Usage="openAIClient.GetCompletionsAsync (deploymentId, prompt, cancellationToken)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>Azure.AI.OpenAI</AssemblyName>
        <AssemblyVersion>1.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Threading.Tasks.Task&lt;Azure.Response&lt;Azure.AI.OpenAI.Completions&gt;&gt;</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="deploymentId" Type="System.String" />
        <Parameter Name="prompt" Type="System.String" />
        <Parameter Name="cancellationToken" Type="System.Threading.CancellationToken" />
      </Parameters>
      <Docs>
        <param name="deploymentId"> Deployment id (also known as model name) to use for operations </param>
        <param name="prompt"> Input string prompt to create a prompt completion from a deployment. </param>
        <param name="cancellationToken"> The cancellation token to use. </param>
        <summary> Return the completion for a given prompt. </summary>
        <returns>To be added.</returns>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <Member MemberName="GetCompletionsStreaming">
      <MemberSignature Language="C#" Value="public virtual Azure.Response&lt;Azure.AI.OpenAI.StreamingCompletions&gt; GetCompletionsStreaming (string deploymentId, Azure.AI.OpenAI.CompletionsOptions completionsOptions, System.Threading.CancellationToken cancellationToken = default);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig newslot virtual instance class Azure.Response`1&lt;class Azure.AI.OpenAI.StreamingCompletions&gt; GetCompletionsStreaming(string deploymentId, class Azure.AI.OpenAI.CompletionsOptions completionsOptions, valuetype System.Threading.CancellationToken cancellationToken) cil managed" />
      <MemberSignature Language="DocId" Value="M:Azure.AI.OpenAI.OpenAIClient.GetCompletionsStreaming(System.String,Azure.AI.OpenAI.CompletionsOptions,System.Threading.CancellationToken)" />
      <MemberSignature Language="VB.NET" Value="Public Overridable Function GetCompletionsStreaming (deploymentId As String, completionsOptions As CompletionsOptions, Optional cancellationToken As CancellationToken = Nothing) As Response(Of StreamingCompletions)" />
      <MemberSignature Language="F#" Value="abstract member GetCompletionsStreaming : string * Azure.AI.OpenAI.CompletionsOptions * System.Threading.CancellationToken -&gt; Azure.Response&lt;Azure.AI.OpenAI.StreamingCompletions&gt;&#xA;override this.GetCompletionsStreaming : string * Azure.AI.OpenAI.CompletionsOptions * System.Threading.CancellationToken -&gt; Azure.Response&lt;Azure.AI.OpenAI.StreamingCompletions&gt;" Usage="openAIClient.GetCompletionsStreaming (deploymentId, completionsOptions, cancellationToken)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>Azure.AI.OpenAI</AssemblyName>
        <AssemblyVersion>1.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>Azure.Response&lt;Azure.AI.OpenAI.StreamingCompletions&gt;</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="deploymentId" Type="System.String" />
        <Parameter Name="completionsOptions" Type="Azure.AI.OpenAI.CompletionsOptions" />
        <Parameter Name="cancellationToken" Type="System.Threading.CancellationToken" />
      </Parameters>
      <Docs>
        <param name="deploymentId">To be added.</param>
        <param name="completionsOptions">To be added.</param>
        <param name="cancellationToken">To be added.</param>
        <summary>To be added.</summary>
        <returns>To be added.</returns>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <Member MemberName="GetCompletionsStreamingAsync">
      <MemberSignature Language="C#" Value="public virtual System.Threading.Tasks.Task&lt;Azure.Response&lt;Azure.AI.OpenAI.StreamingCompletions&gt;&gt; GetCompletionsStreamingAsync (string deploymentId, Azure.AI.OpenAI.CompletionsOptions completionsOptions, System.Threading.CancellationToken cancellationToken = default);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig newslot virtual instance class System.Threading.Tasks.Task`1&lt;class Azure.Response`1&lt;class Azure.AI.OpenAI.StreamingCompletions&gt;&gt; GetCompletionsStreamingAsync(string deploymentId, class Azure.AI.OpenAI.CompletionsOptions completionsOptions, valuetype System.Threading.CancellationToken cancellationToken) cil managed" />
      <MemberSignature Language="DocId" Value="M:Azure.AI.OpenAI.OpenAIClient.GetCompletionsStreamingAsync(System.String,Azure.AI.OpenAI.CompletionsOptions,System.Threading.CancellationToken)" />
      <MemberSignature Language="VB.NET" Value="Public Overridable Function GetCompletionsStreamingAsync (deploymentId As String, completionsOptions As CompletionsOptions, Optional cancellationToken As CancellationToken = Nothing) As Task(Of Response(Of StreamingCompletions))" />
      <MemberSignature Language="F#" Value="abstract member GetCompletionsStreamingAsync : string * Azure.AI.OpenAI.CompletionsOptions * System.Threading.CancellationToken -&gt; System.Threading.Tasks.Task&lt;Azure.Response&lt;Azure.AI.OpenAI.StreamingCompletions&gt;&gt;&#xA;override this.GetCompletionsStreamingAsync : string * Azure.AI.OpenAI.CompletionsOptions * System.Threading.CancellationToken -&gt; System.Threading.Tasks.Task&lt;Azure.Response&lt;Azure.AI.OpenAI.StreamingCompletions&gt;&gt;" Usage="openAIClient.GetCompletionsStreamingAsync (deploymentId, completionsOptions, cancellationToken)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>Azure.AI.OpenAI</AssemblyName>
        <AssemblyVersion>1.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Threading.Tasks.Task&lt;Azure.Response&lt;Azure.AI.OpenAI.StreamingCompletions&gt;&gt;</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="deploymentId" Type="System.String" />
        <Parameter Name="completionsOptions" Type="Azure.AI.OpenAI.CompletionsOptions" />
        <Parameter Name="cancellationToken" Type="System.Threading.CancellationToken" />
      </Parameters>
      <Docs>
        <param name="deploymentId">To be added.</param>
        <param name="completionsOptions">To be added.</param>
        <param name="cancellationToken">To be added.</param>
        <summary>To be added.</summary>
        <returns>To be added.</returns>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <Member MemberName="GetEmbeddings">
      <MemberSignature Language="C#" Value="public virtual Azure.Response&lt;Azure.AI.OpenAI.Embeddings&gt; GetEmbeddings (string deploymentId, Azure.AI.OpenAI.EmbeddingsOptions embeddingsOptions, System.Threading.CancellationToken cancellationToken = default);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig newslot virtual instance class Azure.Response`1&lt;class Azure.AI.OpenAI.Embeddings&gt; GetEmbeddings(string deploymentId, class Azure.AI.OpenAI.EmbeddingsOptions embeddingsOptions, valuetype System.Threading.CancellationToken cancellationToken) cil managed" />
      <MemberSignature Language="DocId" Value="M:Azure.AI.OpenAI.OpenAIClient.GetEmbeddings(System.String,Azure.AI.OpenAI.EmbeddingsOptions,System.Threading.CancellationToken)" />
      <MemberSignature Language="VB.NET" Value="Public Overridable Function GetEmbeddings (deploymentId As String, embeddingsOptions As EmbeddingsOptions, Optional cancellationToken As CancellationToken = Nothing) As Response(Of Embeddings)" />
      <MemberSignature Language="F#" Value="abstract member GetEmbeddings : string * Azure.AI.OpenAI.EmbeddingsOptions * System.Threading.CancellationToken -&gt; Azure.Response&lt;Azure.AI.OpenAI.Embeddings&gt;&#xA;override this.GetEmbeddings : string * Azure.AI.OpenAI.EmbeddingsOptions * System.Threading.CancellationToken -&gt; Azure.Response&lt;Azure.AI.OpenAI.Embeddings&gt;" Usage="openAIClient.GetEmbeddings (deploymentId, embeddingsOptions, cancellationToken)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>Azure.AI.OpenAI</AssemblyName>
        <AssemblyVersion>1.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>Azure.Response&lt;Azure.AI.OpenAI.Embeddings&gt;</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="deploymentId" Type="System.String" />
        <Parameter Name="embeddingsOptions" Type="Azure.AI.OpenAI.EmbeddingsOptions" />
        <Parameter Name="cancellationToken" Type="System.Threading.CancellationToken" />
      </Parameters>
      <Docs>
        <param name="deploymentId"> deployment id of the deployed model. </param>
        <param name="embeddingsOptions"> Schema to create a prompt completion from a deployment. </param>
        <param name="cancellationToken"> The cancellation token to use. </param>
        <summary> Return the embeddings for a given prompt. </summary>
        <returns>To be added.</returns>
        <remarks>To be added.</remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="deploymentId" /> or <paramref name="embeddingsOptions" /> is null. </exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="deploymentId" /> is an empty string, and was expected to be non-empty. </exception>
      </Docs>
    </Member>
    <Member MemberName="GetEmbeddings">
      <MemberSignature Language="C#" Value="public virtual Azure.Response GetEmbeddings (string deploymentId, Azure.Core.RequestContent content, Azure.RequestContext context = default);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig newslot virtual instance class Azure.Response GetEmbeddings(string deploymentId, class Azure.Core.RequestContent content, class Azure.RequestContext context) cil managed" />
      <MemberSignature Language="DocId" Value="M:Azure.AI.OpenAI.OpenAIClient.GetEmbeddings(System.String,Azure.Core.RequestContent,Azure.RequestContext)" />
      <MemberSignature Language="VB.NET" Value="Public Overridable Function GetEmbeddings (deploymentId As String, content As RequestContent, Optional context As RequestContext = Nothing) As Response" />
      <MemberSignature Language="F#" Value="abstract member GetEmbeddings : string * Azure.Core.RequestContent * Azure.RequestContext -&gt; Azure.Response&#xA;override this.GetEmbeddings : string * Azure.Core.RequestContent * Azure.RequestContext -&gt; Azure.Response" Usage="openAIClient.GetEmbeddings (deploymentId, content, context)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>Azure.AI.OpenAI</AssemblyName>
        <AssemblyVersion>1.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>Azure.Response</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="deploymentId" Type="System.String" />
        <Parameter Name="content" Type="Azure.Core.RequestContent" />
        <Parameter Name="context" Type="Azure.RequestContext" />
      </Parameters>
      <Docs>
        <param name="deploymentId"> deployment id of the deployed model. </param>
        <param name="content"> The content to send as the body of the request. Details of the request body schema are in the Remarks section below. </param>
        <param name="context"> The request context, which can override default behaviors of the client pipeline on a per-call basis. </param>
        <summary> Return the embeddings for a given prompt. </summary>
        <returns> The response returned from the service. Details of the response body schema are in the Remarks section below. </returns>
        <remarks>
Below is the JSON schema for the request and response payloads.

Request Body:

Schema for <c>EmbeddingsOptions</c>:
<code>{
  user: string, # Optional. The ID of the end-user, for use in tracking and rate-limiting.
  input_type: string, # Optional. input type of embedding search to use
  model: string, # Optional. ID of the model to use
  input: Union, # Required. Input text to get embeddings for, encoded as a string.
To get embeddings for multiple inputs in a single request, pass an array of strings.
Each input must not exceed 2048 tokens in length.

Unless you are embedding code, we suggest replacing newlines (\n) in your input with a single space,
as we have observed inferior results when newlines are present.
}
</code>

Response Body:

Schema for <c>Embeddings</c>:
<code>{
  object: Literal, # Required. Type of the data field
  data: [
    {
      object: Literal, # Required. Name of the field in which the embedding is contained
      embedding: [number], # Required. List of embeddings value for the input prompt. These represents a measurement of releated of text strings
      index: number, # Required. Index of the prompt to which the EmbeddingItem corresponds
    }
  ], # Required. Embedding values for the prompts submitted in the request
  model: string, # Optional. ID of the model to use
  usage: {
    prompt_tokens: number, # Required. Number of tokens sent in the original request
    total_tokens: number, # Required. Total number of tokens transacted in this request/response
  }, # Required. Usage counts for tokens input using the embeddings API
}
</code></remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="deploymentId" /> or <paramref name="content" /> is null. </exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="deploymentId" /> is an empty string, and was expected to be non-empty. </exception>
        <exception cref="T:Azure.RequestFailedException"> Service returned a non-success status code. </exception>
        <example>
This sample shows how to call GetEmbeddings with required parameters and request content, and how to parse the result.
<code><![CDATA[
var credential = new AzureKeyCredential("<key>");
var endpoint = new Uri("<https://my-service.azure.com>");
var client = new OpenAIClient(endpoint, credential);

var data = new {
    input = new {},
};

Response response = client.GetEmbeddings("<deploymentId>", RequestContent.Create(data));

JsonElement result = JsonDocument.Parse(response.ContentStream).RootElement;
Console.WriteLine(result.GetProperty("object").ToString());
Console.WriteLine(result.GetProperty("data")[0].GetProperty("object").ToString());
Console.WriteLine(result.GetProperty("data")[0].GetProperty("embedding")[0].ToString());
Console.WriteLine(result.GetProperty("data")[0].GetProperty("index").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("prompt_tokens").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("total_tokens").ToString());
]]></code>
This sample shows how to call GetEmbeddings with all parameters and request content, and how to parse the result.
<code><![CDATA[
var credential = new AzureKeyCredential("<key>");
var endpoint = new Uri("<https://my-service.azure.com>");
var client = new OpenAIClient(endpoint, credential);

var data = new {
    user = "<user>",
    input_type = "<input_type>",
    model = "<model>",
    input = new {},
};

Response response = client.GetEmbeddings("<deploymentId>", RequestContent.Create(data));

JsonElement result = JsonDocument.Parse(response.ContentStream).RootElement;
Console.WriteLine(result.GetProperty("object").ToString());
Console.WriteLine(result.GetProperty("data")[0].GetProperty("object").ToString());
Console.WriteLine(result.GetProperty("data")[0].GetProperty("embedding")[0].ToString());
Console.WriteLine(result.GetProperty("data")[0].GetProperty("index").ToString());
Console.WriteLine(result.GetProperty("model").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("prompt_tokens").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("total_tokens").ToString());
]]></code></example>
      </Docs>
    </Member>
    <Member MemberName="GetEmbeddingsAsync">
      <MemberSignature Language="C#" Value="public virtual System.Threading.Tasks.Task&lt;Azure.Response&lt;Azure.AI.OpenAI.Embeddings&gt;&gt; GetEmbeddingsAsync (string deploymentId, Azure.AI.OpenAI.EmbeddingsOptions embeddingsOptions, System.Threading.CancellationToken cancellationToken = default);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig newslot virtual instance class System.Threading.Tasks.Task`1&lt;class Azure.Response`1&lt;class Azure.AI.OpenAI.Embeddings&gt;&gt; GetEmbeddingsAsync(string deploymentId, class Azure.AI.OpenAI.EmbeddingsOptions embeddingsOptions, valuetype System.Threading.CancellationToken cancellationToken) cil managed" />
      <MemberSignature Language="DocId" Value="M:Azure.AI.OpenAI.OpenAIClient.GetEmbeddingsAsync(System.String,Azure.AI.OpenAI.EmbeddingsOptions,System.Threading.CancellationToken)" />
      <MemberSignature Language="VB.NET" Value="Public Overridable Function GetEmbeddingsAsync (deploymentId As String, embeddingsOptions As EmbeddingsOptions, Optional cancellationToken As CancellationToken = Nothing) As Task(Of Response(Of Embeddings))" />
      <MemberSignature Language="F#" Value="abstract member GetEmbeddingsAsync : string * Azure.AI.OpenAI.EmbeddingsOptions * System.Threading.CancellationToken -&gt; System.Threading.Tasks.Task&lt;Azure.Response&lt;Azure.AI.OpenAI.Embeddings&gt;&gt;&#xA;override this.GetEmbeddingsAsync : string * Azure.AI.OpenAI.EmbeddingsOptions * System.Threading.CancellationToken -&gt; System.Threading.Tasks.Task&lt;Azure.Response&lt;Azure.AI.OpenAI.Embeddings&gt;&gt;" Usage="openAIClient.GetEmbeddingsAsync (deploymentId, embeddingsOptions, cancellationToken)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>Azure.AI.OpenAI</AssemblyName>
        <AssemblyVersion>1.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Threading.Tasks.Task&lt;Azure.Response&lt;Azure.AI.OpenAI.Embeddings&gt;&gt;</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="deploymentId" Type="System.String" />
        <Parameter Name="embeddingsOptions" Type="Azure.AI.OpenAI.EmbeddingsOptions" />
        <Parameter Name="cancellationToken" Type="System.Threading.CancellationToken" />
      </Parameters>
      <Docs>
        <param name="deploymentId"> deployment id of the deployed model. </param>
        <param name="embeddingsOptions"> Schema to create a prompt completion from a deployment. </param>
        <param name="cancellationToken"> The cancellation token to use. </param>
        <summary> Return the embeddings for a given prompt. </summary>
        <returns>To be added.</returns>
        <remarks>To be added.</remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="deploymentId" /> or <paramref name="embeddingsOptions" /> is null. </exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="deploymentId" /> is an empty string, and was expected to be non-empty. </exception>
      </Docs>
    </Member>
    <Member MemberName="GetEmbeddingsAsync">
      <MemberSignature Language="C#" Value="public virtual System.Threading.Tasks.Task&lt;Azure.Response&gt; GetEmbeddingsAsync (string deploymentId, Azure.Core.RequestContent content, Azure.RequestContext context = default);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig newslot virtual instance class System.Threading.Tasks.Task`1&lt;class Azure.Response&gt; GetEmbeddingsAsync(string deploymentId, class Azure.Core.RequestContent content, class Azure.RequestContext context) cil managed" />
      <MemberSignature Language="DocId" Value="M:Azure.AI.OpenAI.OpenAIClient.GetEmbeddingsAsync(System.String,Azure.Core.RequestContent,Azure.RequestContext)" />
      <MemberSignature Language="VB.NET" Value="Public Overridable Function GetEmbeddingsAsync (deploymentId As String, content As RequestContent, Optional context As RequestContext = Nothing) As Task(Of Response)" />
      <MemberSignature Language="F#" Value="abstract member GetEmbeddingsAsync : string * Azure.Core.RequestContent * Azure.RequestContext -&gt; System.Threading.Tasks.Task&lt;Azure.Response&gt;&#xA;override this.GetEmbeddingsAsync : string * Azure.Core.RequestContent * Azure.RequestContext -&gt; System.Threading.Tasks.Task&lt;Azure.Response&gt;" Usage="openAIClient.GetEmbeddingsAsync (deploymentId, content, context)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>Azure.AI.OpenAI</AssemblyName>
        <AssemblyVersion>1.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Threading.Tasks.Task&lt;Azure.Response&gt;</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="deploymentId" Type="System.String" />
        <Parameter Name="content" Type="Azure.Core.RequestContent" />
        <Parameter Name="context" Type="Azure.RequestContext" />
      </Parameters>
      <Docs>
        <param name="deploymentId"> deployment id of the deployed model. </param>
        <param name="content"> The content to send as the body of the request. Details of the request body schema are in the Remarks section below. </param>
        <param name="context"> The request context, which can override default behaviors of the client pipeline on a per-call basis. </param>
        <summary> Return the embeddings for a given prompt. </summary>
        <returns> The response returned from the service. Details of the response body schema are in the Remarks section below. </returns>
        <remarks>
Below is the JSON schema for the request and response payloads.

Request Body:

Schema for <c>EmbeddingsOptions</c>:
<code>{
  user: string, # Optional. The ID of the end-user, for use in tracking and rate-limiting.
  input_type: string, # Optional. input type of embedding search to use
  model: string, # Optional. ID of the model to use
  input: Union, # Required. Input text to get embeddings for, encoded as a string.
To get embeddings for multiple inputs in a single request, pass an array of strings.
Each input must not exceed 2048 tokens in length.

Unless you are embedding code, we suggest replacing newlines (\n) in your input with a single space,
as we have observed inferior results when newlines are present.
}
</code>

Response Body:

Schema for <c>Embeddings</c>:
<code>{
  object: Literal, # Required. Type of the data field
  data: [
    {
      object: Literal, # Required. Name of the field in which the embedding is contained
      embedding: [number], # Required. List of embeddings value for the input prompt. These represents a measurement of releated of text strings
      index: number, # Required. Index of the prompt to which the EmbeddingItem corresponds
    }
  ], # Required. Embedding values for the prompts submitted in the request
  model: string, # Optional. ID of the model to use
  usage: {
    prompt_tokens: number, # Required. Number of tokens sent in the original request
    total_tokens: number, # Required. Total number of tokens transacted in this request/response
  }, # Required. Usage counts for tokens input using the embeddings API
}
</code></remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="deploymentId" /> or <paramref name="content" /> is null. </exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="deploymentId" /> is an empty string, and was expected to be non-empty. </exception>
        <exception cref="T:Azure.RequestFailedException"> Service returned a non-success status code. </exception>
        <example>
This sample shows how to call GetEmbeddingsAsync with required parameters and request content, and how to parse the result.
<code><![CDATA[
var credential = new AzureKeyCredential("<key>");
var endpoint = new Uri("<https://my-service.azure.com>");
var client = new OpenAIClient(endpoint, credential);

var data = new {
    input = new {},
};

Response response = await client.GetEmbeddingsAsync("<deploymentId>", RequestContent.Create(data));

JsonElement result = JsonDocument.Parse(response.ContentStream).RootElement;
Console.WriteLine(result.GetProperty("object").ToString());
Console.WriteLine(result.GetProperty("data")[0].GetProperty("object").ToString());
Console.WriteLine(result.GetProperty("data")[0].GetProperty("embedding")[0].ToString());
Console.WriteLine(result.GetProperty("data")[0].GetProperty("index").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("prompt_tokens").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("total_tokens").ToString());
]]></code>
This sample shows how to call GetEmbeddingsAsync with all parameters and request content, and how to parse the result.
<code><![CDATA[
var credential = new AzureKeyCredential("<key>");
var endpoint = new Uri("<https://my-service.azure.com>");
var client = new OpenAIClient(endpoint, credential);

var data = new {
    user = "<user>",
    input_type = "<input_type>",
    model = "<model>",
    input = new {},
};

Response response = await client.GetEmbeddingsAsync("<deploymentId>", RequestContent.Create(data));

JsonElement result = JsonDocument.Parse(response.ContentStream).RootElement;
Console.WriteLine(result.GetProperty("object").ToString());
Console.WriteLine(result.GetProperty("data")[0].GetProperty("object").ToString());
Console.WriteLine(result.GetProperty("data")[0].GetProperty("embedding")[0].ToString());
Console.WriteLine(result.GetProperty("data")[0].GetProperty("index").ToString());
Console.WriteLine(result.GetProperty("model").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("prompt_tokens").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("total_tokens").ToString());
]]></code></example>
      </Docs>
    </Member>
    <Member MemberName="Pipeline">
      <MemberSignature Language="C#" Value="public virtual Azure.Core.Pipeline.HttpPipeline Pipeline { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class Azure.Core.Pipeline.HttpPipeline Pipeline" />
      <MemberSignature Language="DocId" Value="P:Azure.AI.OpenAI.OpenAIClient.Pipeline" />
      <MemberSignature Language="VB.NET" Value="Public Overridable ReadOnly Property Pipeline As HttpPipeline" />
      <MemberSignature Language="F#" Value="member this.Pipeline : Azure.Core.Pipeline.HttpPipeline" Usage="Azure.AI.OpenAI.OpenAIClient.Pipeline" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>Azure.AI.OpenAI</AssemblyName>
        <AssemblyVersion>1.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>Azure.Core.Pipeline.HttpPipeline</ReturnType>
      </ReturnValue>
      <Docs>
        <summary> The HTTP pipeline for sending and receiving REST requests and responses. </summary>
        <value>To be added.</value>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
  </Members>
</Type>
